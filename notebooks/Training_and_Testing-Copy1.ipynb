{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72806b08",
   "metadata": {},
   "source": [
    "# About this Notebook\n",
    "\n",
    "This notebook can be used for training and testing the neural network.\n",
    "\n",
    "It relies on the classes and functions defined in *Classes_and_Functions.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a914d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import cv2  # OpenCV for adaptive filtering\n",
    "import psutil  # For system resource management\n",
    "from scipy.ndimage import convolve  # To convolve filtering masks\n",
    "\n",
    "# PyTorch specific imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c1c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Classes_and_Functions.ipynb\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Notebooks\n",
    "import import_ipynb\n",
    "from Classes_and_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997527e",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "First, define the hyperparameters of which dataset to use, what filter to apply, how Sub-DSIs shall be constructed and the whether to use the single or the multi-pixel version of the network. More options exist for the dataset, see *Classes_and_Functions.ipynb*\n",
    "\n",
    "Quick overview:\n",
    "* Everything can be left at default except the path for the <b>dsi_directory</b> and the <b>depthmap_directory</b>. \n",
    "* The default is the single-pixel version of the network, to use the multi-pixel version set <b>multi_pixel=True</b>.\n",
    "* The process is set to MVSEC stereo on default. If desired, switch to <b>dataset=\"mvsec_mono\"</b> or <b>dataset=\"dsec\"</b>.\n",
    "* The filter parameters are set to default, but for MVSEC, we used <b>filter_size=9</b> and an <b>adaptive_threshold_c=-10</b> for training and testing instead. Feel free to replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73be9215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameters for the dataset:\n",
    "    # DSI Selection Arguments\n",
    "    dataset (str): The dataset used.\n",
    "    test_seq (int): Sequence for testing.\n",
    "    train_seq_A, train_seq_B (str): Sequences for training.\n",
    "    dsi_directory (str): Directory of the DSIs. Must be adjusted to user.\n",
    "    depthmap_directory (str): Directory of the groundtrue depths for each DSI.\n",
    "    dsi_split (str or int)\n",
    "    dsi_split (str or int): Which DSIs shall be considered.\n",
    "                            Can be \"all\", \"even\", \"odd\" or a number between 0 and 9, refering to the last digit of its id.\n",
    "    dsi_ratio (float): Between 0 and 1. Defines the proportion of (random) DSIs that shall be used.\n",
    "    start_idx, end_idx (str): Start and stop indices for which DSIs to consider. \n",
    "    start_row, end_row, start_col, end_col (str): Define the rows and columns to be considered within each DSI.\n",
    "\n",
    "    # Pixel selection\n",
    "    filter_size (int): Determines the size of the neighbourhood area when applying the adaptive threshold filter.\n",
    "    adaptive_threshold_c (int): Constant that is subtracted from the mean of the neighbourhood pixels when apply the adaptive threshold filter.\n",
    "\n",
    "    # Sub-DSIs sizes\n",
    "    sub_frame_radius_h (int): Defines the radius of the frame at the height axis around the central pixel for the Sub-DSI.\n",
    "    sub_frame_radius_w (int): Defines the radius of the frame at the width axis around the central pixel for the Sub-DSI.\n",
    "\n",
    "    # Network version\n",
    "    multi_pixel (bool): Determines whether depth is predicted only for the central selected pixel or for the 8 neighbouring pixels as well.\n",
    "\"\"\"\n",
    "\n",
    "# Dataset selection\n",
    "dataset = \"mvsec_stereo\" #  Options: mvsec_stereo, mvsec_mono, dsec\n",
    "test_seq = 1  #  Options: 1,2,3 (only for MVSEC sequence)\n",
    "train_seq_A, train_seq_B = {1,2,3} - {test_seq}\n",
    "\n",
    "# Directories\n",
    "dsi_directory_test = f\"/mnt/RIPHD4TB/diego/data/mvsec/indoor_flying{test_seq}/dsi_stereo/\" #  Set your path here\n",
    "dsi_directory_train_A = f\"/mnt/RIPHD4TB/diego/data/mvsec/indoor_flying{train_seq_A}/dsi_stereo/\"\n",
    "dsi_directory_train_B = f\"/mnt/RIPHD4TB/diego/data/mvsec/indoor_flying{train_seq_B}/dsi_stereo/\"\n",
    "depthmap_directory_test = f\"/mnt/RIPHD4TB/diego/data/mvsec/indoor_flying{test_seq}/depthmaps/\" #  Set your path here\n",
    "depthmap_directory_train_A = f\"/mnt/RIPHD4TB/diego/data/mvsec/indoor_flying{train_seq_A}/depthmaps/\"\n",
    "depthmap_directory_train_B = f\"/mnt/RIPHD4TB/diego/data/mvsec/indoor_flying{train_seq_B}/depthmaps/\"\n",
    "\n",
    "# dsi_split\n",
    "split = \"even\"\n",
    "dsi_split_test = split #  Options: all, even, odd, 0, 1, ..., 9\n",
    "dsi_split_train_A = split\n",
    "dsi_split_train_B = split\n",
    "\n",
    "# dsi_ratio\n",
    "dsi_ratio_test = 1.0 #  Options: 0 < dsi_ratio <= 1\n",
    "dsi_ratio_train_A = 1.0\n",
    "dsi_ratio_train_B = 1.0\n",
    "\n",
    "# start_idx and end_idx\n",
    "start_idx_test, end_idx_test = 140-5, 1201-5 #  0, None \n",
    "start_idx_train_A, end_idx_train_A = 160-5, 1580-5 #  0, None\n",
    "start_idx_train_B, end_idx_train_B = 125-5, 1815-5 #  0, None\n",
    "\n",
    "# start and end idx of rows and columns\n",
    "start_row_test, end_row_test = 0, None\n",
    "start_col_test, end_col_test = 0, None\n",
    "start_row_train_A, end_row_train_A = 0, None\n",
    "start_col_train_A, end_col_train_A = 0, None\n",
    "start_row_train_B, end_row_train_B = 0, None\n",
    "start_col_train_B, end_col_train_B = 0, None\n",
    "\n",
    "# Filter parameters for pixel selection\n",
    "filter_size_test = None #  None automatically sets default value. We used 9 for training and testing on MVSEC instead\n",
    "filter_size_train_A = 9 #  9 for MVSEC\n",
    "filter_size_train_B = 9 #  9 for MVSEC\n",
    "adaptive_threshold_c_test = None #  None automatically sets default value. We used -10 for MVSEC instead\n",
    "adaptive_threshold_c_train_A = -10 #  -10 for MVSEC\n",
    "adaptive_threshold_c_train_B = -10 #  -10 for MVSEC\n",
    "\n",
    "# Sub-DSI sizes\n",
    "sub_frame_radius_h = 3\n",
    "sub_frame_radius_w = 3\n",
    "\n",
    "# Network version\n",
    "multi_pixel = False #  Set to True for multi-pixel network version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b22d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If DSEC was selected as dataset, decide below whether which half to use for training and testing.\n",
    "# middle_idx is set to the middle of the index for the zurich_city04a sequence, but can be set to a different custom value as well.\n",
    "if dataset == \"dsec\":\n",
    "    # For DSEC training and test sets can be split by divining one sequence\n",
    "    middle_idx = 174\n",
    "    # First half being used for training and second half for testing.\n",
    "    start_idx_test = middle_idx\n",
    "    end_idx_train_A = middle_idx\n",
    "    # Out-comment the 2 lines above and un-comment the 2 lines below to reverse order \n",
    "    \"\"\"\n",
    "    end_idx_test = middle_idx\n",
    "    start_idx_train_A = middle_idx\n",
    "    \"\"\"\n",
    "    # A second training set is not needed for DSEC\n",
    "    end_idx_train_B = start_idx_train_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d8f1d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e6f7b",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711ac6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load DSI 195\n",
      "Load DSI 341\n",
      "Load DSI 421\n",
      "Load DSI 465\n",
      "Load DSI 557\n",
      "Load DSI 657\n",
      "Load DSI 667\n",
      "Load DSI 741\n",
      "Load DSI 1061\n",
      "Load DSI 1249\n",
      "Load DSI 1533\n",
      "\n",
      "Load DSI 168\n",
      "Load DSI 242\n",
      "Load DSI 300\n",
      "Load DSI 390\n",
      "Load DSI 800\n",
      "Load DSI 1256\n",
      "Load DSI 1588\n",
      "Load DSI 1654\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "# Decide whether the progress of reading in the DSIs shall be printed for tracking\n",
    "print_progress = True\n",
    "\n",
    "# Create training data\n",
    "training_data_A = DSI_Pixelswise_Dataset(dataset=dataset,\n",
    "                                         data_seq=train_seq_A,\n",
    "                                         dsi_directory=dsi_directory_train_A,\n",
    "                                         depthmap_directory=depthmap_directory_train_A,\n",
    "                                         dsi_split=dsi_split_train_A,\n",
    "                                         dsi_ratio=dsi_ratio_train_A,\n",
    "                                         start_idx=start_idx_train_A, end_idx=end_idx_train_A,\n",
    "                                         start_row=start_row_train_A, end_row=end_row_train_A,\n",
    "                                         start_col=start_col_train_A, end_col=end_col_train_A,\n",
    "                                         filter_size=filter_size_train_A,\n",
    "                                         adaptive_threshold_c=adaptive_threshold_c_train_A,\n",
    "                                         sub_frame_radius_h=sub_frame_radius_h,\n",
    "                                         sub_frame_radius_w=sub_frame_radius_w,\n",
    "                                         multi_pixel=multi_pixel,\n",
    "                                         clip_targets=True, #  Clip depths for training\n",
    "                                         print_progress=print_progress\n",
    "                                        )\n",
    "\n",
    "if print_progress: print(\"\")\n",
    "\n",
    "training_data_B = DSI_Pixelswise_Dataset(dataset=dataset,\n",
    "                                         data_seq=train_seq_B,\n",
    "                                         dsi_directory=dsi_directory_train_B,\n",
    "                                         depthmap_directory=depthmap_directory_train_B,\n",
    "                                         dsi_split=dsi_split_train_B,\n",
    "                                         dsi_ratio=dsi_ratio_train_B,\n",
    "                                         start_idx=start_idx_train_B, end_idx=end_idx_train_B,\n",
    "                                         start_row=start_row_train_B, end_row=end_row_train_B,\n",
    "                                         start_col=start_col_train_B, end_col=end_col_train_B,\n",
    "                                         filter_size=filter_size_train_B,\n",
    "                                         adaptive_threshold_c=adaptive_threshold_c_train_B,\n",
    "                                         sub_frame_radius_h=sub_frame_radius_h,\n",
    "                                         sub_frame_radius_w=sub_frame_radius_w,\n",
    "                                         multi_pixel=multi_pixel,\n",
    "                                         clip_targets=True,\n",
    "                                         print_progress=print_progress\n",
    "                                        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a21596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training data\n",
    "training_data = ConcatDataset([training_data_A, training_data_B])\n",
    "# Inherit some attributes\n",
    "training_data.dataset = training_data_A.dataset\n",
    "training_data.pixel_count = training_data_A.pixel_count + training_data_B.pixel_count\n",
    "training_data.frame_height, training_data.frame_width = training_data_A.frame_height, training_data_A.frame_width\n",
    "training_data.min_depth, training_data. max_depth = training_data_A.min_depth, training_data_B. max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03070787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data into Dataloader\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cde1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data A size: 20317\n",
      "training data B size: 17679\n",
      "training data size: 37996\n",
      "pixel number for inference: 41738\n",
      "sub dsi size: torch.Size([100, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# Print data dimensions\n",
    "training_data_A_size = len(training_data_A)\n",
    "training_data_B_size = len(training_data_B)\n",
    "training_data_size = len(training_data)\n",
    "sub_dsi_size = training_data_A.data_list[0][1].shape\n",
    "\n",
    "print(\"training data A size:\", training_data_A_size)\n",
    "print(\"training data B size:\", training_data_B_size)\n",
    "print(\"training data size:\", training_data_size)\n",
    "print(\"pixel number for inference:\", training_data.pixel_count)\n",
    "print(\"sub dsi size:\", sub_dsi_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c17fa",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "\n",
    "More options exist for the network architecture, see *Classes_and_Functions.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d6822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixelwiseConvGRU(\n",
      "  (conv3d): Sequential(\n",
      "    (0): Conv3d(1, 4, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(1, 0, 0))\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "  )\n",
      "  (gru): GRU(100, 100, batch_first=True)\n",
      "  (dense_output): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = PixelwiseConvGRU(sub_frame_radius_h, sub_frame_radius_w, multi_pixel=multi_pixel)\n",
    "# Send to cuda\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# Print architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ef5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditions for the training process\n",
    "epochs = 5 #  3\n",
    "data_augmentation = False #  data_augmentation randomly inverts DSIs on horizontally and/or vertically\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = CustomMAELoss() if multi_pixel else torch.nn.L1Loss() # CustomMAELoss is L1Loss which ignores  NaN-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, an previous trained version of the model can be loaded. To do so, give the file name and uncomment this cell.\n",
    "\"\"\"\n",
    "previous_model_path = \"example_path\" #  Set your path here\n",
    "previous_model_file = \"example_file\" #  Set your file name here\n",
    "model.load_parameters(previous_model_file, device=device, model_path=previous_model_path, optimizer=optimizer)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393808d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to store model in directory\n",
    "model_path = f\"/mnt/RIPHD4TB/diego/models/mvsec/indoor_flying{test_seq}/\" #  \"example_path\"\n",
    "# Define name of model file\n",
    "model_file = f\"{split}_model\" #  \"example_model\"\n",
    "# In the training process, the current epoch will be added to each files name\n",
    "# Therefore do NOT set \".pth\"\n",
    "if model_file.endswith(\".pth\"):\n",
    "    model_file = model_file[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c88ff",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1563d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, data, model, loss_fn, optimizer, data_augmentation=False):\n",
    "    \"\"\"\n",
    "    Function to define the training process.\n",
    "    The data instance itself is needed to derive hyperparameters of the dataset.\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Get size of entire dataset\n",
    "    data_size = len(dataloader.dataset)\n",
    "    # Get number of batches\n",
    "    num_batches = len(dataloader)\n",
    "    save_points = [int(num_batches * (i / 5)) for i in range(1, 5+1)]\n",
    "    # Account for single or multi pixel network-version\n",
    "    num_estims = 9 if model.multi_pixel else 1\n",
    "    # Track estimates and true depths\n",
    "    epoch_network_estimates = torch.zeros(num_estims * data_size, dtype=torch.float32, device=device)\n",
    "    epoch_argmax_estimates = torch.zeros(num_estims * data_size, dtype=torch.float32, device=device)\n",
    "    epoch_true_depths = torch.zeros(num_estims * data_size, dtype=torch.float32, device=device)\n",
    "    # Track current index for these tensors\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    save_batch = 0\n",
    "    for batch, batch_data in enumerate(dataloader):\n",
    "        # If available, use GPU (device has to be set earlier)\n",
    "        batch_data = tuple(tensor.to(device) for tensor in batch_data)\n",
    "        # Get batch data\n",
    "        pixel_position, sub_dsi, true_depth, argmax_depth, frame_idx = batch_data\n",
    "        batch_size = true_depth.size(0)\n",
    "        # Train on batch and return network prediction (without augmented predictions)\n",
    "        pred = train_batch(batch_data, model, loss_fn, optimizer, data_augmentation=data_augmentation)\n",
    "        # Clip network estimations to inbetween 0 and 1\n",
    "        network_depth = pred.clip(0,1)\n",
    "        # Update epoch estimates and target values\n",
    "        epoch_network_estimates[current_idx:current_idx + num_estims * batch_size] = network_depth.flatten()\n",
    "        epoch_argmax_estimates[current_idx:current_idx + num_estims * batch_size] = argmax_depth.repeat_interleave(num_estims)\n",
    "        epoch_true_depths[current_idx:current_idx + num_estims * batch_size] = true_depth.flatten()\n",
    "        # Update index\n",
    "        current_idx += num_estims * batch_size\n",
    "        \n",
    "        # Save model\n",
    "        if (batch + 1) in save_points:\n",
    "            save_batch += 1\n",
    "            checkpoint_file = f\"{epoch_model_file[:-4]}_batch_{save_batch}.pth\"\n",
    "            model.save_model(optimizer, checkpoint_file, model_path=model_path, print_save=True)\n",
    "        # Clear memory cache\n",
    "        gc.collect()\n",
    "            \n",
    "    # Compute and print performance for epoch\n",
    "    evaluate_performance(data, epoch_network_estimates, epoch_argmax_estimates, epoch_true_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f77fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_1_batch_1.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_1_batch_2.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_1_batch_3.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_1_batch_4.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_1_batch_5.pth\n",
      "Network Test Error Performance:\n",
      " MAE: 31.07 | MedAE: 15.31 | Bad Pix: 8.47 | SILog: 4.88 | ARE: 15.00 | log RMSE: 22.10 | delta1: 79.55 | delta2: 93.39 | delta3: 98.21 | #Pix: 41738\n",
      "Argmax Test Error Performance:\n",
      " MAE: 22.90 | MedAE: 9.08 | Bad Pix: 2.80 | SILog: 2.63 | ARE: 9.39 | log RMSE: 16.22 | delta1: 92.10 | delta2: 96.90 | delta3: 98.58 | #: 41738\n",
      "\n",
      "Training time [min]: 4.0\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_2_batch_1.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_2_batch_2.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_2_batch_3.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_2_batch_4.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_2_batch_5.pth\n",
      "Network Test Error Performance:\n",
      " MAE: 16.77 | MedAE: 7.57 | Bad Pix: 1.89 | SILog: 1.67 | ARE: 7.46 | log RMSE: 12.92 | delta1: 94.71 | delta2: 97.96 | delta3: 99.29 | #Pix: 41738\n",
      "Argmax Test Error Performance:\n",
      " MAE: 22.90 | MedAE: 9.08 | Bad Pix: 2.80 | SILog: 2.63 | ARE: 9.39 | log RMSE: 16.22 | delta1: 92.10 | delta2: 96.90 | delta3: 98.58 | #: 41738\n",
      "\n",
      "Training time [min]: 3.0\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_3_batch_1.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_3_batch_2.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_3_batch_3.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_3_batch_4.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_3_batch_5.pth\n",
      "Network Test Error Performance:\n",
      " MAE: 15.51 | MedAE: 6.72 | Bad Pix: 1.81 | SILog: 1.55 | ARE: 6.88 | log RMSE: 12.45 | delta1: 95.11 | delta2: 98.01 | delta3: 99.35 | #Pix: 41738\n",
      "Argmax Test Error Performance:\n",
      " MAE: 22.90 | MedAE: 9.08 | Bad Pix: 2.80 | SILog: 2.63 | ARE: 9.39 | log RMSE: 16.22 | delta1: 92.10 | delta2: 96.90 | delta3: 98.58 | #: 41738\n",
      "\n",
      "Training time [min]: 3.0\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_4_batch_1.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_4_batch_2.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_4_batch_3.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_4_batch_4.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_4_batch_5.pth\n",
      "Network Test Error Performance:\n",
      " MAE: 14.75 | MedAE: 6.20 | Bad Pix: 1.73 | SILog: 1.46 | ARE: 6.54 | log RMSE: 12.10 | delta1: 95.39 | delta2: 98.13 | delta3: 99.36 | #Pix: 41738\n",
      "Argmax Test Error Performance:\n",
      " MAE: 22.90 | MedAE: 9.08 | Bad Pix: 2.80 | SILog: 2.63 | ARE: 9.39 | log RMSE: 16.22 | delta1: 92.10 | delta2: 96.90 | delta3: 98.58 | #: 41738\n",
      "\n",
      "Training time [min]: 4.0\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_5_batch_1.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_5_batch_2.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_5_batch_3.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_5_batch_4.pth\n",
      "Saved PyTorch Model and Optimizer State to E:/diego_models/mvsec/indoor_flying1/even_model_epoch_5_batch_5.pth\n",
      "Network Test Error Performance:\n",
      " MAE: 14.35 | MedAE: 5.97 | Bad Pix: 1.70 | SILog: 1.42 | ARE: 6.38 | log RMSE: 11.94 | delta1: 95.52 | delta2: 98.12 | delta3: 99.38 | #Pix: 41738\n",
      "Argmax Test Error Performance:\n",
      " MAE: 22.90 | MedAE: 9.08 | Bad Pix: 2.80 | SILog: 2.63 | ARE: 9.39 | log RMSE: 16.22 | delta1: 92.10 | delta2: 96.90 | delta3: 98.58 | #: 41738\n",
      "\n",
      "Training time [min]: 6.0\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Start the training process\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    # Add epoch to file name\n",
    "    epoch_model_file = f\"{model_file}_epoch_{epoch+1}.pth\"\n",
    "    # Train and track time\n",
    "    st = time.time()\n",
    "    train(train_dataloader, training_data, model, loss_fn, optimizer, data_augmentation=data_augmentation)\n",
    "    ct = time.time()\n",
    "    print(\"\\n\", \"Training time [min]: \", (ct-st)//60, sep=\"\")\n",
    "    # Save\n",
    "    #model.save_model(optimizer, epoch_model_file, model_path=model_path, print_save=True) #  Set print_save to False to not print message\n",
    "    print(\"\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01977c81",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc788d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b33410aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load DSI 175\n",
      "Load DSI 321\n",
      "Load DSI 401\n",
      "Load DSI 445\n",
      "Load DSI 537\n",
      "Load DSI 637\n",
      "Load DSI 647\n",
      "Load DSI 721\n",
      "Load DSI 1041\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "# Decide whether the progress of reading in the DSIs shall be printed for tracking\n",
    "print_progress = True\n",
    "\n",
    "# Create testset\n",
    "test_data = DSI_Pixelswise_Dataset(dataset=dataset,\n",
    "                                   data_seq=test_seq,\n",
    "                                   dsi_directory=dsi_directory_test,\n",
    "                                   depthmap_directory=depthmap_directory_test,\n",
    "                                   dsi_split=dsi_split_test,\n",
    "                                   dsi_ratio=dsi_ratio_test,\n",
    "                                   start_idx=start_idx_test, end_idx=end_idx_test,\n",
    "                                   start_row=start_row_test, end_row=end_row_test,\n",
    "                                   start_col=start_col_test, end_col=end_col_test,\n",
    "                                   filter_size=filter_size_test,\n",
    "                                   adaptive_threshold_c=adaptive_threshold_c_test,\n",
    "                                   sub_frame_radius_h=sub_frame_radius_h,\n",
    "                                   sub_frame_radius_w=sub_frame_radius_w,\n",
    "                                   multi_pixel=multi_pixel,\n",
    "                                   clip_targets=False, #  Do not clip depths for testing\n",
    "                                   print_progress=print_progress\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "048b44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data into Dataloader\n",
    "batch_size = 64\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04a39f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data size: 10991\n",
      "pixel number for inference: 12322\n",
      "sub dsi size: torch.Size([100, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# Print data dimensions\n",
    "test_data_size = len(test_data)\n",
    "sub_dsi_size = test_data.data_list[0][1].shape\n",
    "\n",
    "print(\"test data size:\", test_data_size)\n",
    "print(\"pixel number for inference:\", test_data.pixel_count)\n",
    "print(\"sub dsi size:\", sub_dsi_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c29daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 2 #  How many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69544f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = [PixelwiseConvGRU(sub_frame_radius_h, sub_frame_radius_w, multi_pixel=multi_pixel) for _ in range(num_models)]\n",
    "# Send to cuda\n",
    "if torch.cuda.is_available():\n",
    "    for model in models:\n",
    "        model.cuda()\n",
    "# Print architecture\n",
    "print(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0bd844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether test data should be inverted horizontally\n",
    "flip_horizontal = False\n",
    "# Decide whether test data should be inverted vertically\n",
    "flip_vertical = False\n",
    "# To rotate the data by 0, 90, 180 or 270 degrees, set rotate to 0, 1, 2 or 3.\n",
    "rotate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34eb211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to load models from directory\n",
    "model_paths = [f\"/mnt/RIPHD4TB/diego/models/mvsec/indoor_flying{test_seq}/\"] * 2#  [\"example_path_A, example_path_B\"]\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    for batch in range(1,5+1):\n",
    "        # Give names of model files\n",
    "        model_files = [f\"even_model_epoch_{epoch}_batch_{batch}.pth\", f\"odd_model_epoch_{epoch}_batch_{batch}.pth\"] #  [\"example_model_A.pth\", \"example_model_B.pth\"]\n",
    "        # Do not forget \".pth\"\n",
    "        for idx, model_file in enumerate(model_files):\n",
    "            if not model_file.endswith(\".pth\"):\n",
    "                model_files[idx] += \".pth\"\n",
    "        # Load models parameters\n",
    "        for idx, model in enumerate(models):\n",
    "        model.load_parameters(model_files[idx], device=device, model_path=model_paths[idx], optimizer=None)\n",
    "        # Use ensemble learning to create averaged model\n",
    "        model = AveragedNetwork(models)\n",
    "        test(test_dataloader, test_data, model, flip_horizontal=flip_horizontal, flip_vertical=flip_vertical, rotate=rotate)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c63f041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Test Error Performance:\n",
      " MAE: 16.96 | MedAE: 9.10 | Bad Pix: 0.76 | SILog: 1.77 | ARE: 6.90 | log RMSE: 13.32 | delta1: 96.55 | delta2: 99.01 | delta3: 99.56 | #Pix: 12322\n",
      "Argmax Test Error Performance:\n",
      " MAE: 22.03 | MedAE: 10.69 | Bad Pix: 1.03 | SILog: 2.37 | ARE: 8.28 | log RMSE: 15.57 | delta1: 94.93 | delta2: 98.34 | delta3: 99.12 | #: 12322\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, test_data, model, flip_horizontal=flip_horizontal, flip_vertical=flip_vertical, rotate=rotate)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
